from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
from pytube import extract

import os
import textwrap
import re
import openai
import urllib

summury_file_path = r"vedio_summary"
openai.api_key = "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"
#OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_ORGANIZATION = os.getenv("OPENAI_ORGANIZATION")

openai.organization = OPENAI_ORGANIZATION
PROMPT_STRING = "Write a detailed summary of the following:\n\n<<SUMMARY>>\n"
# openai.api_key = OPENAI_API_KEY
url= input("Enter your url")
#url='https://youtu.be/kqtD5dpn9C8'
video_id=extract.video_id(url)

#summary_file_path = r"vedio_summary"
#openai.api_key = "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"
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
#OPENAI_ORGANIZATION = os.getenv("OPENAI_ORGANIZATION")

#openai.organization = OPENAI_ORGANIZATION
# openai.api_key = OPENAI_API_KEY

transcript = YouTubeTranscriptApi.get_transcript(video_id)
#   title = transcript['items'][0]['snippet']['title']

# Format transcript using TextFormatter from youtube_transcript_api library
formatter = TextFormatter()
transcript = formatter.format_transcript(transcript)

video_length = len(transcript)

# If the video is ~25 minutes or more, double the chunk size
# This is done to reduce overall amount of API calls
chunk_size = 4000 if video_length >= 25000 else 2000

# Wrap the transcript in chunks of characters
chunks = textwrap.wrap(transcript, chunk_size)

summaries = list()

# For each chunk of characters, generate a summary
for chunk in chunks:
    prompt = PROMPT_STRING.replace("<<SUMMARY>>", chunk)

# Generate summary using GPT-3
# If the davinci model is incurring too much cost, 
# the text-curie-001 model may be used in its place.
response = openai.Completion.create(
model="text-davinci-003", prompt=prompt, max_tokens=256
)
summary = re.sub("\s+", " ", response.choices[0].text.strip())
summaries.append(summary)

# Join the chunk summaries into one string
chunk_summaries = " ".join(summaries)
prompt = PROMPT_STRING.replace("<<SUMMARY>>", chunk_summaries)

# Generate a final summary from the chunk summaries
response = openai.Completion.create(
model="text-davinci-003", prompt=prompt, max_tokens=2056
)
final_summary = re.sub("\s+", " ", response.choices[0].text.strip())

# Print out all of the summaries
for idx, summary in enumerate(summaries):
    print(f"({idx}) - {summary}\n")

#youtube_summury_file = summury_file_path.replace(os.path.splitext(summury_file_path)[1], "_summary.txt")
youtube_summury_file = summury_file_path
with open(youtube_summury_file, "w+") as file:
    file.write(final_summary)
#print(f"(Final Summary) - {final_summary}")
